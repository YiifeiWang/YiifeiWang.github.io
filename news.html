<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>News</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Yifei Wang</div>
<div class="menu-item"><a href="index.html" class="current">Home</a></div>
<div class="menu-item"><a href="news.html">News</a></div>
<div class="menu-category">Publications</div>
<div class="menu-item"><a href="publications_topic.html">sorted&nbsp;by&nbsp;topics</a></div>
<div class="menu-item"><a href="publications_date.html">sorted&nbsp;by&nbsp;dates</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>News</h1>
</div>
<ul>
<li><p><b>2025/02</b> I defended my dissertation! The slide can be found <a href="https://yiifeiwang.github.io/files/Yifei_s_Oral_Defense.pdf">here</a>.</p>
</li>
</ul>
<ul>
<li><p><b>2025/01</b> Our <a href="https://arxiv.org/abs/2209.15265">paper</a> <b>Overparameterized ReLU Neural Networks Learn the Simplest Models: Neural Isometry and Exact Recovery</b> is accepted for IEEE transactions on Information Theory.</p>
</li>
</ul>
<ul>
<li><p><b>2024/06</b> I start to intern at Apple ASM group. Thanks a lot to Frank and Minda for hosting me.</p>
</li>
</ul>
<ul>
<li><p><b>2024/06</b> Our <a href="https://arxiv.org/abs/2402.00220">paper</a> <b>A Circuit Approach to Constructing Blockchains on Blockchains</b> is accepted at <a href="https://aftconf.github.io/aft24/index.html">AFT&rsquo;24</a>. Too long; Don't read (TLDR): we build a more secure overlay blockchain by reading from and writing to a given set of blockchains.</p>
</li>
</ul>
<ul>
<li><p><b>2024/03</b> Our paper <b>A Library of Mirrors: Deep Neural Nets in Low Dimensions are Convex Lasso Models with Reflection Features</b> is available on <a href="https://arxiv.org/pdf/2403.01046.pdf">arxiv</a>. Too long; Don't read (TLDR): training a neural network on 1D dataset is equivalent to solving a Lasso problem. This extends to deep neural networks up to 4 layer. </p>
</li>
</ul>
<ul>
<li><p><b>2024/02</b> Our paper <b>A Circuit Approach to Constructing Blockchains on Blockchains</b> is available on <a href="https://arxiv.org/abs/2402.00220">arxiv</a>. TLDR: we build a more secure overlay blockchain by reading from and writing to a given set of blockchains. </p>
</li>
</ul>
<ul>
<li><p><b>2023/10</b> Our paper <b>Polynomial-Time Solutions for ReLU Network Training: A Complexity Classification via Max-Cut and Zonotopes</b> is available on <a href="https://arxiv.org/abs/2311.10972">arxiv</a>. TLDR: we use max-cut and zonotope to provide a classification on the difficulty of training two-layer ReLU neural networks. </p>
</li>
</ul>
<ul>
<li><p><b>2023/9</b> I finished my internship at Babylon Chain. Thanks a lot to David and Sankha for hosting me. My main responsibilities encompassed user behavior analysis of Bitcoin, an extensive survey on the Proof of Stake (PoS) liquid staking model and their related incentive programs, and a thorough tokenomics investigation of emerging blockchains, including Akash. </p>
</li>
</ul>
<ul>
<li><p><b>2023/4</b> Our <a href="https://arxiv.org/pdf/2210.12212.pdf">paper</a> <b>Sketching the Krylov Subspace: Faster Computation of the Entire Ridge Regularization path</b> with <a href="https://github.com/pilancilab/IHS-BIN">code</a> is accepted for the Journal of Supercomputing 2023. Too long; Don't read (TLDR): we use polynomial expansion and iterative Hessian sketch to compute the entire regularzation path of ridge regression.</p>
</li>
</ul>
<ul>
<li><p><b>2023/2</b> Our <a href="https://arxiv.org/abs/2109.11707">paper</a> <b>A Decomposition Augmented Lagrangian Method for Low-rank Semidefinite Programming</b> is to appear on SIAM on Optimization (2023). We provide a fast solvers for general non-smooth semi-definite programs with low-rank structure.</p>
</li>
</ul>
<ul>
<li><p><b>2023/1</b> Our <a href="http://web.stanford.edu/~wangyf18/files/parallel_deep_neural.pdf">paper</a> <b>Parallel Deep Neural Networks Have Zero Duality Gap</b> is accepted for 2023 as a poster. In short, we make the convex duality gap for deep neural networks become zero by considering the parallel neural network structure. To a parallel neural network with m branches, the output is the linear combinations of the outputs of m standard fully connected neural networks. </p>
</li>
</ul>
<ul>
<li><p><b>2022/12</b> I implement the range Bits Back Coding as the final project for <a href="https://stanforddatacompressionclass.github.io/Fall22/">EE274 Data Compression</a>. <b>TL;DR: We compress the multi-set to the information limit!</b> Checkout the <a href="http://web.stanford.edu/~wangyf18/files/EE274_report.pdf">report</a> and <a href="https://github.com/YiifeiWang/Bits-back-Coding">code</a>!</p>
</li>
</ul>
<ul>
<li><p><b>2022/09</b> Our new paper <b>Overparameterized ReLU Neural Networks Learn the Simplest Models: Neural Isometry and Exact Recovery</b> is available on <a href="https://arxiv.org/abs/2209.15265">Arxiv</a> and the codes are available on <a href="https://github.com/pilancilab/Neural-recovery">Github</a>. We resolve the discrepancy between the remarkable generalization and model complexity from a convex optimization and sparse recovery perspective. Under certain regularity assumptions on the data, we show that ReLU networks with an arbitrary number of parameters learn only simple models that explain the data.</p>
</li>
</ul>
<ul>
<li><p><b>2022/09</b> I gave a presentation of our <a href="https://arxiv.org/pdf/2205.13098.pdf">preprint</a> on <b></b>Optimal Neural Network Approximation of Wasserstein Gradient Direction via Convex Optimization<b></b> at <a href="https://siammds22.us2.pathable.com/meetings/virtual/hAcND2raXY7cYvG2m">SIAM MDS22</a> at San Deigo. <b>TL;DR: How Wasserstein Gradient Flow meets Neural Networks and Convex Optimization?</b></p>
</li>
</ul>
<ul>
<li><p><b>2022/09</b> Our paper <b>Beyond the Best: Distribution Functional Estimation in Infinite-Armed Bandits</b> is accepted in NeurIPS 2022! We provide offline and online sample complexity of estimating mean, quantile, trimmed mean and maximum from noisy observations. We develop a unified meta algorithms and prove the general information-theoretical lower bounds for both offline and online sampling.</p>
</li>
</ul>
<div id="footer">
<div id="footer-text">
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
