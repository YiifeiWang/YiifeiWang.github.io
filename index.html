<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Yifei Wang</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Yifei Wang</div>
<div class="menu-item"><a href="index.html" class="current">Home</a></div>
<div class="menu-item"><a href="news.html">News</a></div>
<div class="menu-category">Publications</div>
<div class="menu-item"><a href="publications_topic.html">sorted&nbsp;by&nbsp;topics</a></div>
<div class="menu-item"><a href="publications_date.html">sorted&nbsp;by&nbsp;dates</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Yifei Wang</h1>
</div>
<table class="imgtable"><tr><td>
<img src="https://yiifeiwang.github.io/myphoto.JPG" alt="Me" width="300px" />&nbsp;</td>
<td align="left"><p>Yifei Wang (<span lang="utf-8" xml:lang="ko"><b>汪祎非</b></span>)<br />
Machine Learning Scientist <br />
Platform Architecture @ <a href="https://www.apple.com">Apple</a>/<br /></p>
</td></tr></table>
<p><br /></p>
<h2>About me</h2>
<p>Currently I work at Apple as a machine learning scientist. I completed mt Ph.D. degress in the Department of Electrical Engineering at Stanford University, where I am coadvised by Prof. <a href="https://stanford.edu/~pilanci/">Mert Pilanci</a> and Prof. <a href="https://tselab.stanford.edu/people/principal-investigator/david-tse/">David Tse</a>. Prior to joining Stanford, I obtained my B.S. degree of Computational and Applied Mathematics in <a href="https://www.math.pku.edu.cn/en/">School of Mathematical Science</a> at <a href="https://english.pku.edu.cn">Peking University</a>, advised by Prof. <a href="https://bicmr.pku.edu.cn/~wenzw/">Zaiwen Wen</a>.</p>
<h2>Recent News</h2>
<ul>
<li><p><b>2025/04</b> I started my job at Apple.</p>
</li>
</ul>
<ul>
<li><p><b>2025/02</b> I defended my dissertation! The slide can be found <a href="http://web.stanford.edu/~wangyf18/files/Yifei_s_Oral_Defense.pdf">here</a>.</p>
</li>
</ul>
<ul>
<li><p><b>2025/01</b> Our <a href="https://arxiv.org/abs/2209.15265">paper</a> <b>Overparameterized ReLU Neural Networks Learn the Simplest Models: Neural Isometry and Exact Recovery</b> is accepted for IEEE transactions on Information Theory.</p>
</li>
</ul>
<ul>
<li><p><b>2024/06</b> I start to intern at Apple ASM group. Thanks a lot to Frank and Minda for hosting me.</p>
</li>
</ul>
<ul>
<li><p><b>2024/06</b> Our <a href="https://arxiv.org/abs/2402.00220">paper</a> <b>A Circuit Approach to Constructing Blockchains on Blockchains</b> is accepted at <a href="https://aftconf.github.io/aft24/index.html">AFT&rsquo;24</a>. Too long; Don't read (TLDR): we build a more secure overlay blockchain by reading from and writing to a given set of blockchains.</p>
</li>
</ul>
<ul>
<li><p><b>2024/06</b> Our paper <b>Randomized Geometric Algebra Methods for Convex Neural Networks</b> is available on <a href="https://arxiv.org/abs/2406.02806">arxiv</a>. Too long; Don't read (TLDR): We introduce randomized algorithms to Clifford's Geometric Algebra for finetuning Large Language Models.</p>
</li>
</ul>
<ul>
<li><p><b>2024/05</b> Our <a href="https://arxiv.org/pdf/2205.13098.pdf">paper</a> <b></b>Optimal Neural Network Approximation of Wasserstein Gradient Direction via Convex Optimization<b></b> is accepted at SIAM Journal on Mathematics of Data Science. Too long; Don't read (TLDR): We apply neural networks and convex optimization to approximate Wasserstein Gradient Direction.</p>
</li>
</ul>
<ul>
<li><p><b>2024/03</b> Our paper <b>A Library of Mirrors: Deep Neural Nets in Low Dimensions are Convex Lasso Models with Reflection Features</b> is available on <a href="https://arxiv.org/pdf/2403.01046.pdf">arxiv</a>. Too long; Don't read (TLDR): training a neural network on 1D dataset is equivalent to solving a Lasso problem. This extends to deep neural networks up to 4 layer.</p>
</li>
</ul>
<h2>Contact</h2>
<p>If you find any of my research interesting or have any questions, feel free to reach out to me via email!</p>
<p>Email &nbsp;&nbsp; <i>wangyf18 at stanford dot edu</i></p>
<p><a href="https://www.linkedin.com/in/yifei-wang-54a180218/">LinkedIn</a> </p>
<p><a href="https://scholar.google.com/citations?user=b4PmtFkAAAAJ&amp;hl=zh-TW">Google scholar</a> </p>
<p><a href="https://github.com/YiifeiWang">My Github</a> </p>
<p><a href="https://github.com/pilancilab">Pilanci Research Group's Github</a></p>
<h2>CV</h2>
<p><a href="https://yiifeiwang.github.io/files/YifeiWang_CV.pdf">My CV</a></p>
<h2>Last update</h2>
<p>May 3rd, 202</p>
<div id="footer">
<div id="footer-text">
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
